\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[nounderscore]{syntax}
\usepackage{listings, listings-rust}

\lstset{tabsize=4}

\renewcommand{\syntleft}{$\langle$\normalfont\ttfamily}
\setlength{\grammarindent}{2cm}
\setlength{\grammarparsep}{0cm}

\title{SPL compiler in Rust report}
\author{Oussama Danba}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
In this report I will describe my SPL compiler written in the programming language Rust. The report will mostly focus on the decisions made while implementing SPL rather than focusing on implementation details.

The report is divided in the sections: Scanning and Parsing, Semantic Analysis, Code Generation, and Compiler Extension. This is done to reflect the course set-up.

\section{Scanning and Parsing}
The first thing to note here is that the scanner (also known as lexer) and parser are two separate entities. My primary reason for doing this is that it allows the parser to abstract from all whitespace since it is handled by the scanner. This would simplify the parser to only deal with the actual contents of the source code. Additionally it allowed me to work on the scanner before the lecture on parsers had occurred.

\subsection{Scanner}
For the sake of completeness the grammar the scanner considers is the one specified below. It is equivalent to the grammar provided except the missing $\langle$char$\rangle$ is added. In this slightly altered grammar a character is considered an alphabetic character surrounded by single quotation marks. Using the specified grammar we know all characters the scanner is supposed to handle.

The scanner is implemented in a recursive manner. Initially it is given the source code of the entire program. The first thing it does is strip all the leading whitespace and then tries to ``scan'' what is left. This either results in a Token that is scanned or it results in an error as the scanner apparently does not recognize the given input and aborts. In the case it resulted in a Token it calls itself recursively with the rest of the input. Given a correct SPL program the scanner results in a list of Tokens.

The idea is simple but there are some implementation details that need to be discussed. The tokens produced by the scanner are written down in Appendix~\ref{Appendix1}. One issue is that of code clarity. Having one large function produce all different types of Tokens makes it unmaintainable. As a result the code going from a String to a Token is split into separate functions which are applied after each other if the previous one fails. The code looks as follows:
\begin{lstlisting}[language=Rust, style=boxed]
let result = scan_comment(trimmed_input)
	.or_else(|| scan_reserved(trimmed_input))
	.or_else(|| scan_int(trimmed_input))
	.or_else(|| scan_bool(trimmed_input))
	.or_else(|| scan_char(trimmed_input))
	.or_else(|| scan_identifiers(trimmed_input));
\end{lstlisting}

Another detail is that of comments. I opted to scan comments as the \texttt{TokenComment(String)} Token as we might want comments later. This ended up not being the case but the functionality is still there.

The order of scanning the tokens is also relevant. In my implementation I've chosen to represent \texttt{::} and \texttt{->} as single Tokens instead of two separate tokens. If \texttt{TokenColon} would be scanned before \texttt{TokenDoubleColon} this would result in a wrong Token. Parsing it as a single Token instead of two separate Tokens is irrelevant for the parser but in my opinion it better separates what the Tokens are used for.

Scanning negative numbers is a little tricky. Initially I scanned negative numbers to the \texttt{TokenInt(i64)} Token with the negative sign included. However this is a problem when making the parser. The expression \texttt{5 -4} is a correct expression but would result in two \texttt{TokenInt(i64)} Tokens. Since there is no binary operator in-between the parser will reject this. As such, I decided to always scan the negative sign as the TokenMinus Token and deal with negative numbers inside the parser rather than the scanner.

One last detail that I feel was not adequately solved is the fact that the recursion is not a tail-recursion. This means that for potentially every character in the source String we have a new stack frame. Even in less extreme cases we result in a lot of stack frames. For longer programs I have experience Windows running out of stack space while Linux does not. This is because Windows has a stack size of 1MB be default while Linux has 8MB. Running into this limit for this course is unlikely but definitely something that I would do differently.

My scanner supports fairly detailed error reporting as we're still working on the source string. It will report the line where it errored and in what context. To make error reporting during parsing possible the scanner does a little bit of extra work and remembers the position a Token what scanned at. The parser uses this to find the context in which a parsing error occurred. An example scanner error can be found below:
\begin{lstlisting}[style=boxed]
Scanner error at line 2 near ':
	return';
\end{lstlisting}

\subsubsection*{Scanner Grammar:}
\begin{grammar}
	<SPL> = <Decl>+

	<Decl> = <VarDecl> \\| <FunDecl>

	<VarDecl> = ('\textbf{var}' | <Type>) <id> '=' <Exp> ';'

	<FunDecl> = <id> '(' [ <FArgs> ] ')' [ '::' <FunType> ] '\{' <VarDecl>* <Stmt>+ '\}'

	<RetType> = <Type> \\| '\textbf{Void}'

	<FunType> = [ <FTypes> ] '$\rightarrow$' <RetType>

	<FTypes> = <Type> [ <FTypes> ]

	<Type> = <BasicType> \\| '(' <Type> ',' <Type> ')' \\| '[' <Type> ']' \\| <id>

	<BasicType> = '\textbf{Int}' \\| '\textbf{Bool}' \\| '\textbf{Char}'

	<FArgs> = [ <FArgs> ',' ] <id>

	<Stmt> = '\textbf{if}' '(' <Exp> ')' '\{' <Stmt>* '\}' [ '\textbf{else}' '\{' <Stmt>* '\}' ] \\| '\textbf{while}' '(' <Exp> ')' '\{' <Stmt>* '\}' \\| <id> <Field> '=' <Exp> ';' \\| <FunCall> ';' \\| '\textbf{return}' [ <Exp> ] ';'

	<Exp> = <id> <Field> \\| <Exp> <Op2> <Exp> \\| <Op1> <Exp> \\| <int> \\| <char> \\| '\textbf{False}' \\| '\textbf{True}' \\| '(' <Exp> ')' \\| <FunCall> \\| '[]' \\| '(' <Exp> ',' <Exp> ')'

	<Field> = [ <Field> ( '.' '\textbf{hd}' | '.' '\textbf{tl}' | '.' '\textbf{fst}' | '.' '\textbf{snd}' ) ]

	<FunCall> = <id> '(' [ <ActArgs> ] ')'

	<ActArgs> = <Exp> [ ',' <ActArgs> ]

	<Op2> = '+' | '-' | '*' | '/' | '\%' | '==' | '\textless' | '\textgreater' | '\textless=' | '\textgreater=' | '!=' | '\&\&' | '||' | ':'

	<Op1> = '!' | '-'

	<int> = [ '-' ] digit+

	<char> = ' alpha '

	<id> = alpha ( '\_' | alphaNum)*
\end{grammar}

\subsection{Parser}
To implement parsing I decided to use recursive descent parsing. In order to do this in Rust I needed a parser combinator library. The two main choices are \texttt{nom} and \texttt{combine}. I've opted to use \texttt{combine} as it closely mirrors Haskell's \texttt{parsec} which I've tried out before. There were a few hurdles to get it working in Rust. The smaller hurdle was that \texttt{combine} is typically used to parse Strings rather than Tokens. This means that some of the default combinators aren't really useful thus requiring me to write my own. The bigger hurdle was that combinators in \texttt{combine} are defined using Rust macros. This means that there are some compile-time text substitutions in the combinators which abstract away a lot of the type information. This tends to result in really cryptic errors since I have no idea what the underlying types are. This took a bit of time to get used to but towards the end I could typically write a combinator in one try. A minor annoyance was that combinators that fail at some point consume Tokens. So a choice combinator that fails on the first choice may consume input which has the effect that the other variants of the choice combinator will never apply. This often requires wrapping combinators in the try combinator to alleviate this issue. The parser that parses identifiers looks as follows:
\begin{lstlisting}[language=Rust, style=boxed]
parser!{
	fn parse_identifier[I]()(I) -> Ident
	  where [I: Stream<Item=Token>]
	{
		satisfy_map(|x: Token| match x {
			Token::TokenIdentifier(ident) => Some(ident),
			_ => None,
		})
	}
}
\end{lstlisting}

Before implementing the parser I first decided to modify the grammar to solve some issues. These issues are operator precedence, associativity, and left-recursion. While the grammar that was parsed was altered this is barely reflected in the Abstract Syntax Tree. The AST is a straightforward data structure on how one would expect an SPL program to look. The altered grammar can be found below while the final AST (slightly modified for Semantic Analysis) can be found in Appendix~\ref{Appendix2}.

The first change in the grammar is that $\langle$BasicType$\rangle$ has been moved into $\langle$Type$\rangle$. This is merely for convenience and doesn't affect the grammar in any way.

A change that does matter is that $\langle$FArgs$\rangle$ has been changed to remove the left-recursion in it. Left-recursion is a problem for recursive descent parsers as it results in infinite recursion since no Tokens are ever consumed. Luckily changing $\langle$FArgs$\rangle$ to remove left-recursion is trivial. A similar left-recursion can be found in $\langle$Field$\rangle$. To remove left-recursion there $\langle$Field$\rangle$ is split into $\langle$Field$\rangle$ and $\langle$Field2$\rangle$ where $\langle$Field$\rangle$ is simply zero or more $\langle$Field2$\rangle$ Tokens.

The last place where left-recursion is still a problem is in $\langle$Exp$\rangle$ but this part of the grammar also needs to be changed to handle operator precedence and associativity. To solve the operator precedence issue I split $\langle$Exp$\rangle$ for binary operators into multiple $\langle$ExpN$\rangle$ where every $\langle$ExpN$\rangle$ has the next expression at the left side. Since the parser is a recursive descent parser it means that the parser will go into the left branch first which has the result that expressions deeper in the ``ladder'' have a higher priority. Building this laddered structure conveniently also solves the left-recursion issue.

The expression at the right side is for the other side of the operator. This structure has the result that every binary operator is right associative. However, some binary operators are left associative (such as subtraction). This is the reason why for some operators we collect multiple right sides. Imagine the expression \texttt{5 - 4 - 3 - 2 1}. When this reaches the parser for $\langle$Exp5$\rangle$ the result will be \texttt{Lit(Int(5))} for the left side and \texttt{[(TokenMinus, Lit(Int(4))), (TokenMinus, Lit(Int(3))), (TokenMinus, Lit(Int(2))), (TokenMinus, Lit(Int(1)))]} for the right side. Since the right side is a list of expressions we can do a right fold which will result in a single expression that is left associative. This also works in cases where the right side has higher precedence (because that will be done first) and where the right side is right associative.

Using the grammar described the recursive descent parser is fully functional. Error reporting during parsing is a little tricky. \texttt{Combine} returns the Token where a parsing error occurs and since the location of a Token is stored during scanning the compiler can report the error and in what context it happens. However, if combine successfully parses a part of the Token stream it does not result in an error. This has changed in \texttt{Combine 3.0} but that is not stable at this point in time. So it may happen that part of the program compiles fine but a function is in reality actually missing because it had an error somewhere. When it does report an error it looks as follows (missing opening \texttt{(}):
\begin{lstlisting}[language=Rust, style=boxed]
Parser error at line 1. Unexpected token TokenComma:
[(Int, (Bool, Char))] x = 1, (True, 'a')) : [];
\end{lstlisting}

\subsubsection*{Parser Grammar:}
\begin{grammar}
	<SPL> = <Decl>+

	<Decl> = <VarDecl> \\| <FunDecl>

	<VarDecl> = ('\textbf{var}' | <Type>) <id> '=' <Exp> ';'

	<FunDecl> = <id> '(' [ <FArgs> ] ')' [ '::' <FunType> ] '\{' <VarDecl>* <Stmt>+ '\}'

	<RetType> = <Type> \\| '\textbf{Void}'

	<FunType> = [ <FTypes> ] '$\rightarrow$' <RetType>

	<FTypes> = <Type> [ <FTypes> ]

	<Type> = '\textbf{Int}' \\| '\textbf{Bool}' \\| '\textbf{Char}' \\| '(' <Type> ',' <Type> ')' \\| '[' <Type> ']' \\| <id>

	<FArgs> = <id> [ <FArgs> ',' ]

	<Stmt> = '\textbf{if}' '(' <Exp> ')' '\{' <Stmt>* '\}' [ '\textbf{else}' '\{' <Stmt>* '\}' ] \\| '\textbf{while}' '(' <Exp> ')' '\{' <Stmt>* '\}' \\| <id> <Field> '=' <Exp> ';' \\| <FunCall> ';' \\| '\textbf{return}' [ <Exp> ] ';'

	<Exp> = <Exp2> [ '||' <Exp> ]

	<Exp2> = <Exp3> [ '\&\&' <Exp2> ]

	<Exp3> = <Exp4> [ ( '==' | '\textless' | '\textgreater' | '\textless=' | '\textgreater=' | '!=' ) <Exp3> ]

	<Exp4> = <Exp5> ( ( '+' | '-' ) <Exp5> )*

	<Exp5> = <Exp6> ( ( '*' | '/' ) <Exp6> )*

	<Exp6> = <Exp7> ( [ '\%' <Exp7> ] )*

	<Exp7> = <Exp8> [ ':' <Exp7> ]

	<Exp8> = <id> <Field> \\| <Op1> <Exp> \\| <int> \\| <char> \\| '\textbf{False}' \\| '\textbf{True}' \\| '(' <Exp> ')' \\| <FunCall> \\| '[]' \\| '(' <Exp> ',' <Exp> ')'

	<Field> = <Field2>*

	<Field2> = '.' '\textbf{hd}' | '.' '\textbf{tl}' | '.' '\textbf{fst}' | '.' '\textbf{snd}'

	<FunCall> = <id> '(' [ <ActArgs> ] ')'

	<ActArgs> = <Exp> [ ',' <ActArgs> ]

	<Op1> = '!' | '-'

	<int> = [ '-' ] digit+

	<char> = ' alpha '

	<id> = alpha ( '\_' | alphaNum)*
\end{grammar}

\subsection{Pretty-printer}
Implementing the pretty printer is fairly straightforward. After parsing the result is an Abstract Syntax Tree. In order to format this nicely I implement the \texttt{Display} trait for every enum/struct in the AST. Implementing the \texttt{Display} trait involves implementing its associated \texttt{fmt} function which described how the data structure should be printed/displayed. So for something simple as the \texttt{Type} enum the implementation looks as follows:
\begin{lstlisting}[language=Rust, style=boxed]
impl fmt::Display for Type {
	fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
		write!(f, "{}", match *self {
			Type::TInt => String::from("Int"),
			Type::TBool => String::from("Bool"),
			Type::TChar => String::from("Char"),
			Type::TVoid => String::from("Void"),
			Type::TTuple(ref l, ref r) =>
				format!("({}, {})", l, r),
			Type::TList(ref t) => format!("[{}]", t),
			Type::TArrow(ref ftypes, ref rtype) =>
				format!("{}-> {}",
				ftypes.iter().map(|x| format!("{} ", x))
				.collect::<Vec<String>>().concat(), rtype)
		})
	}
}
\end{lstlisting}
For the basic types it simply prints the keyword whereas for composed types it prints the structure around the composed types and then pretty prints the composed types. The result is a nicely printed type.

The most difficult part is managing indentation and ensuring that there are no excessive brackets/braces. Thus for pretty printing the operators which are left associative and the precedence of operators are stored somewhere rather than only describing them through the grammar. The most difficult part is when operators are chained. Currently this is implemented by checking the left and right side of a binary operator and seeing which side has a higher precedence and what the associativity of each side is. The result is a pretty-printer that does indenting and puts braces whenever necessary.

\section{Semantic Analysis}
Smenatic Analysis consists of binding time analysis and type checking/inference. For type checking/inference I decided to start with monomorphic type checking and potentially extend to polymorphic type checking if time allowed me to. Unfortunately this did not end up happening and I stuck with monomorphic type checking with exceptions for print and isEmpty in order to make the language somewhat useful. The flipside is that monomorphic type checking works very well.

In order to only support monomorphic type checking the grammar had to be changed somewhat. For variable declarations the \texttt{var} keyword is no longer present, function types are no longer optional, and identifiers are removed from the $\langle$Type$\rangle$ construct. This also resulted in minor changes to the AST as some of the \texttt{Type} enums no longer have to be wrapped in \texttt{Option} since they must always exist. Additionally \texttt{TIdent(String)} was removed from the \texttt{Type} enum. The updated grammar can be found below while the final AST can be found in Appendix~\ref{Appendix2}.

For binding time analysis my compiler makes use of a type environment. This type environment stores the names of functions and variables and their corresponding type. Whenever a variable is shadowed by a newer variable declaration it is replaced in the type environment. As a result an identifier only has to be checked for existence in the type environment to determine whether it is in scope or not. The type stored is used to make sure the type of the identifier fits in the context it is used in. A bit of extra care has to be taken whenever a variable is shadowed since one function shadowing a variable should not affect another function not shadowing this variable.

The type checker has a specific order in which it works. It first scans all the function headers and checks if they are correct (correct amount of parameters with the specified types). If they are correct they are entered into the type environment. This can be done without information on anything else and which is why it occurs first.

After that all global variables are checked. A global variable is checked by making sure the specified type at the left side matches the type the right side returns. This has to be done after entering functions into the type environment since expressions at the right side might have function calls which have to be checked.

Checking expressions is fairly easy. If an identifier expression is found it simply checks if the identifier is in the type environment and returns the type. If a binary operator expression is found we simply check if the types of the left and right sides are expected for that binary operator (thus handling overloading). A function call expression is checked by checking all arguments of the function call and seeing if the resulting types of those match the types of the function call. Such procedures are implemented for every expression which has the result that every expression results in a type which can then be used later. One thing that should be noted is that the type of every expression is also separately stored in a \texttt{HashMap} where the key is a pointer to the expression. This is very useful when generating code later.

After checking global variables and entering them into the type environment we can finally check the function bodies as function bodies may rely on global variable declarations. The first thing that is done is that variables are shadowed in the type environment. Variable shadowing does not result in any warning whatsoever as I consider it a normal part of every programming language. After the variable shadowing, and thus checking the variable declarations, the statement are checked.

The easy statements to check are variable assignments, function calls, and return statements. For variable assignments the compiler simply checks if the left side has the same type as the right side. For function calls the same procedure as expressions is taking (in fact the code simply calls the function that analysis expressions). Checking return statements simply involves checking if the type of the expression in the return statement is the same as the return type of the function with some special handling when the return has no expression.

The difficult statements to check are the if/else and while statements. They are difficult because the compiler must ensure that all code paths must result in a return at some point. The procedure to check if/else statements works as follows: The expression within the if is checked to see if it returns a type Bool. Then the contents of the if and else branch are separately checked. Additionally to only checking every branch, the compiler also records if the branch results in a guaranteed return or not. If and only if both branches guarantee a return then the whole if/else statement guarantees a return. This even works when if/else statements are nested. By doing it this way the compiler can know at the end of a function whether there was a guaranteed return beforehand or not. If there wasn't then the compiler will complain that not all paths have a return value. The exception to this is a return with no expression as the function has a return of type Void in that case. Unfortunately this guaranteeing can not be used for while statements. The following code would say that there was a guaranteed return unless the compiler evaluates the expression within the while but that is not possible in all cases. Thus for while statements the compiler simply treats them as never returning.
\begin{lstlisting}[language=Rust, style=boxed]
while(False) {
	return;
}
\end{lstlisting}

After checking the statements in the function bodies type checking is done. If no error occurred anywhere the SPL program is considered correct, otherwise it outputs the error and why it occurs. Two example errors would be:
\begin{lstlisting}[language=Rust, style=boxed]
Global variable type mismatch in:
(Int, Bool) y = x.tl;
Given type '(Int, Bool)', found type '[(Int, Bool)]'.

Return statement 'return;' has type 'Void' while function
expects type 'Int'.
\end{lstlisting}

Something to note about error handling in general. Every stage of the compiler increasingly works on data that is further from the original source code making it harder to have good error messages. In the scanner it is still working directly on the source string. In the parser it is working on Tokens which I annotated with positions for error reporting. During semantic analysis it works on the AST which has no relation to the original source code anymore. To get somewhat around this the pretty-printer is used to print the context.

\subsubsection*{Semantic Analysis Grammar:}
\begin{grammar}
	<SPL> = <Decl>+

	<Decl> = <VarDecl> \\| <FunDecl>

	<VarDecl> = <Type> <id> '=' <Exp> ';'

	<FunDecl> = <id> '(' [ <FArgs> ] ')' '::' <FunType> '\{' <VarDecl>* <Stmt>+ '\}'

	<RetType> = <Type> \\| '\textbf{Void}'

	<FunType> = [ <FTypes> ] '$\rightarrow$' <RetType>

	<FTypes> = <Type> [ <FTypes> ]

	<Type> = '\textbf{Int}' \\| '\textbf{Bool}' \\| '\textbf{Char}' \\| '(' <Type> ',' <Type> ')' \\| '[' <Type> ']'

	<FArgs> = <id> [ <FArgs> ',' ]

	<Stmt> = '\textbf{if}' '(' <Exp> ')' '\{' <Stmt>* '\}' [ '\textbf{else}' '\{' <Stmt>* '\}' ] \\| '\textbf{while}' '(' <Exp> ')' '\{' <Stmt>* '\}' \\| <id> <Field> '=' <Exp> ';' \\| <FunCall> ';' \\| '\textbf{return}' [ <Exp> ] ';'

	<Exp> = <Exp2> [ '||' <Exp> ]

	<Exp2> = <Exp3> [ '\&\&' <Exp2> ]

	<Exp3> = <Exp4> [ ( '==' | '\textless' | '\textgreater' | '\textless=' | '\textgreater=' | '!=' ) <Exp3> ]

	<Exp4> = <Exp5> ( ( '+' | '-' ) <Exp5> )*

	<Exp5> = <Exp6> ( ( '*' | '/' ) <Exp6> )*

	<Exp6> = <Exp7> ( [ '\%' <Exp7> ] )*

	<Exp7> = <Exp8> [ ':' <Exp7> ]

	<Exp8> = <id> <Field> \\| <Op1> <Exp> \\| <int> \\| <char> \\| '\textbf{False}' \\| '\textbf{True}' \\| '(' <Exp> ')' \\| <FunCall> \\| '[]' \\| '(' <Exp> ',' <Exp> ')'

	<Field> = <Field2>*

	<Field2> = '.' '\textbf{hd}' | '.' '\textbf{tl}' | '.' '\textbf{fst}' | '.' '\textbf{snd}'

	<FunCall> = <id> '(' [ <ActArgs> ] ')'

	<ActArgs> = <Exp> [ ',' <ActArgs> ]

	<Op1> = '!' | '-'

	<int> = [ '-' ] digit+

	<char> = ' alpha '

	<id> = alpha ( '\_' | alphaNum)*
\end{grammar}

\section{Code Generation}
% This might be unexpected due to the linked lists
% [Int] q = 5 : [];
% [Int] x = 1 : 2 : q;
% q.hd = 0;

% mention the existence of RuntimeErr

% == looks at whether the values are the same instead of only the pointers

% mention evaluation order (of function calls / operators)
% exp1 || exp2 always does both in my code generator!

\clearpage
\appendix
\section{Scanner Tokens}
\label{Appendix1}
\begin{lstlisting}[language=Rust, style=boxed]
pub enum Token {
	TokenIdentifier(String),
	TokenInt(i64),
	TokenBool(bool),
	TokenChar(char),
	TokenComment(String),
	TokenVar,
	TokenVoid,
	TokenIntKeyword,
	TokenBoolKeyword,
	TokenCharKeyword,
	TokenIf,
	TokenElse,
	TokenWhile,
	TokenReturn,
	TokenArrow,
	TokenDoubleColon,
	TokenDoubleEquals,
	TokenLtEquals,
	TokenGtEquals,
	TokenNotEquals,
	TokenDoubleAmp,
	TokenDoublePipe,
	TokenEquals,
	TokenParenOpen,
	TokenParenClose,
	TokenBracketOpen,
	TokenBracketClose,
	TokenBraceOpen,
	TokenBraceClose,
	TokenColon,
	TokenSemiColon,
	TokenComma,
	TokenDot,
	TokenPlus,
	TokenMinus,
	TokenMult,
	TokenDiv,
	TokenMod,
	TokenLt,
	TokenGt,
	TokenNot
}
\end{lstlisting}
\section{Final Abstract Syntax Tree}
\label{Appendix2}
\begin{lstlisting}[language=Rust, style=boxed]
pub type Ident = String;

pub struct SPL {
	pub vars: Vec<Variable>,
	pub funs: Vec<Function>
}

pub struct Variable {
	pub name: Ident,
	pub vtype: Type,
	pub value: Expression
}

pub struct Function {
	pub name: Ident,
	pub args: Vec<Ident>,
	pub ftype: Type,
	pub vars: Vec<Variable>,
	pub stmts: Vec<Statement>
}

pub enum Type {
	TInt,
	TBool,
	TChar,
	TVoid,
	TTuple(Box<Type>, Box<Type>),
	TList(Box<Type>),
	TArrow(Vec<Type>, Box<Type>)
}

pub enum Statement {
	If(Expression, Vec<Statement>, Vec<Statement>),
	While(Expression, Vec<Statement>),
	Assignment(Ident, Vec<Field>, Expression),
	FunCall(Expression),
	Return(Option<Expression>)
}

pub enum Expression {
	Ident(Ident, Vec<Field>),
	Op2(Box<Expression>, Op2, Box<Expression>),
	Op1(Op1, Box<Expression>),
	Lit(Literal),
	FunCall(Ident, Vec<Expression>),
	Tuple(Box<Expression>, Box<Expression>)
}

pub enum Field {
	Head,
	Tail,
	First,
	Second
}

pub enum Op2 {
	Addition,
	Subtraction,
	Multiplication,
	Division,
	Modulo,
	Equals,
	LessThan,
	GreaterThan,
	LessEquals,
	GreaterEquals,
	NotEquals,
	And,
	Or,
	Cons
}

pub enum Op1 {
	Not,
	Negation
}

pub enum Literal {
	Int(i64),
	Char(char),
	Bool(bool),
	EmptyList
}
\end{lstlisting}
\end{document}
