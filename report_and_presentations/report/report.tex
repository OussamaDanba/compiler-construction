\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[nounderscore]{syntax}
\usepackage{listings, listings-rust}

\lstset{tabsize=4}

\renewcommand{\syntleft}{$\langle$\normalfont\ttfamily}
\setlength{\grammarindent}{2cm}
\setlength{\grammarparsep}{0cm}

\title{SPL compiler in Rust report}
\author{Oussama Danba --- s4435591\\Cyber Security specialization}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
In this report I will describe my SPL compiler written in the programming language Rust. The report will mostly focus on the decisions made while implementing SPL rather than focusing on implementation details.

The report is divided in the sections: Scanning and Parsing, Semantic Analysis, Code Generation, and Compiler Extension. This is done to reflect the course set-up.

This report is accompanied with some basic tests for the compiler. What each test is supposed to test can be found inside the test.

\section{Rust and Statistics}
Before the course began I knew we were allowed to use any language we preferred and that we would cover recursive descent parsing. I briefly considered using a functional programming language (either Clean or Haskell) but decided against it as I do not consider myself comfortable enough in those languages. I frequently struggle with coming up with an alternative solution in those languages if my initial solution does not work. As such, I wanted something more imperative. Out of the more imperative languages my favorite language for a while has been Rust. It is a low-level programming language with no runtime system (no garbage collector or JIT) and is able to provide guarantees that code is safe at compile-time (no buffer overflows, no memory corruption, data-race free, \dots). The compiler provides really useful error messages, the package manager is incredibly useful, and the language has a lot of functional aspects (higher order functions, lazy evaluation, nested functions, pattern matching, \dots). I've been working with Rust for a while now but haven't had the chance to do a larger project yet. Writing the compiler in Rust seemed to be an ideal project.

Now for some statistics about the compiler. The scanner is about 200 lines of code and took slightly more than a work day to make (about 10 hours). The parser is about 400 lines of code and took about three days to complete (with another day for fixing some minor bugs and making the pretty-printer) so I would estimate that at 28 hours. Semantic analysis is about the same size at around 400 lines of code. That took about two full days of work (10-11 hours a day; I tend to prefer to work in bulk) and another slower day for a total of about 25 hours. The code generator is larger than the previous ones with about 500 lines of code. It also ended up taking most of the time with three days of actual work and another day spent thinking about the structure of everything. I would estimate that at 30 hours.

% extension still to be done

Besides working on the compiler there is also time spent changing older things (I've changed the operator precedence half a dozen of times), thinking about the assignment, making presentations, and making this report. All in all I do think I spent about 6 EC of time into this course which I found surprising as I'm working alone.

% still 6 EC?

\section{Scanning and Parsing}
The first thing to note here is that the scanner (also known as lexer) and parser are two separate entities. My primary reason for doing this is that it allows the parser to abstract from all whitespace since it is handled by the scanner. This would simplify the parser to only deal with the actual contents of the source code. Additionally it allowed me to work on the scanner before the lecture on parsers had occurred.

\subsection{Scanner}
For the sake of completeness the grammar the scanner considers is the one specified below. It is equivalent to the grammar provided except the missing $\langle$char$\rangle$ is added. In this slightly altered grammar a character is considered an alphabetic character surrounded by single quotation marks. Using the specified grammar we know all characters the scanner is supposed to handle.

The scanner is implemented in a recursive manner. Initially it is given the source code of the entire program. The first thing it does is strip all the leading whitespace and then tries to ``scan'' what is left. This either results in a Token that is scanned or it results in an error as the scanner apparently does not recognize the given input and aborts. In the case it resulted in a Token it calls itself recursively with the rest of the input. Given a correct SPL program the scanner results in a list of Tokens.

The idea is simple but there are some implementation details that need to be discussed. The tokens produced by the scanner are written down in Appendix~\ref{Appendix1}. One issue is that of code clarity. Having one large function produce all different types of Tokens makes it unmaintainable. As a result the code going from a String to a Token is split into separate functions which are applied after each other if the previous one fails. The code looks as follows:
\begin{lstlisting}[language=Rust, style=boxed]
let result = scan_comment(trimmed_input)
	.or_else(|| scan_reserved(trimmed_input))
	.or_else(|| scan_int(trimmed_input))
	.or_else(|| scan_bool(trimmed_input))
	.or_else(|| scan_char(trimmed_input))
	.or_else(|| scan_identifiers(trimmed_input));
\end{lstlisting}

Another detail is that of comments. I opted to scan comments as the \texttt{TokenComment(String)} Token as we might want comments later. This ended up not being the case but the functionality is still there.

The order of scanning the tokens is also relevant. In my implementation I've chosen to represent \texttt{::} and \texttt{->} as single Tokens instead of two separate tokens. If \texttt{TokenColon} would be scanned before \texttt{TokenDoubleColon} this would result in a wrong Token. Parsing it as a single Token instead of two separate Tokens is irrelevant for the parser but in my opinion it better separates what the Tokens are used for.

Scanning negative numbers is a little tricky. Initially I scanned negative numbers to the \texttt{TokenInt(i64)} Token with the negative sign included. However this is a problem when making the parser. The expression \texttt{5 -4} is a correct expression but would result in two \texttt{TokenInt(i64)} Tokens. Since there is no binary operator in-between the parser will reject this. As such, I decided to always scan the negative sign as the TokenMinus Token and deal with negative numbers inside the parser rather than the scanner.

One last detail that I feel was not adequately solved is the fact that the recursion is not a tail-recursion. This means that for potentially every character in the source String we have a new stack frame. Even in less extreme cases we result in a lot of stack frames. For longer programs I have experience Windows running out of stack space while Linux does not. This is because Windows has a stack size of 1MB be default while Linux has 8MB. Running into this limit for this course is unlikely but definitely something that I would do differently.

My scanner supports fairly detailed error reporting as we're still working on the source string. It will report the line where it errored and in what context. To make error reporting during parsing possible the scanner does a little bit of extra work and remembers the position a Token what scanned at. The parser uses this to find the context in which a parsing error occurred. An example scanner error can be found below:
\begin{lstlisting}[style=boxed]
Scanner error at line 2 near ':
	return';
\end{lstlisting}

\subsubsection*{Scanner Grammar:}
\begin{grammar}
	<SPL> = <Decl>+

	<Decl> = <VarDecl> \\| <FunDecl>

	<VarDecl> = ('\textbf{var}' | <Type>) <id> '=' <Exp> ';'

	<FunDecl> = <id> '(' [ <FArgs> ] ')' [ '::' <FunType> ] '\{' <VarDecl>* <Stmt>+ '\}'

	<RetType> = <Type> \\| '\textbf{Void}'

	<FunType> = [ <FTypes> ] '$\rightarrow$' <RetType>

	<FTypes> = <Type> [ <FTypes> ]

	<Type> = <BasicType> \\| '(' <Type> ',' <Type> ')' \\| '[' <Type> ']' \\| <id>

	<BasicType> = '\textbf{Int}' \\| '\textbf{Bool}' \\| '\textbf{Char}'

	<FArgs> = [ <FArgs> ',' ] <id>

	<Stmt> = '\textbf{if}' '(' <Exp> ')' '\{' <Stmt>* '\}' [ '\textbf{else}' '\{' <Stmt>* '\}' ] \\| '\textbf{while}' '(' <Exp> ')' '\{' <Stmt>* '\}' \\| <id> <Field> '=' <Exp> ';' \\| <FunCall> ';' \\| '\textbf{return}' [ <Exp> ] ';'

	<Exp> = <id> <Field> \\| <Exp> <Op2> <Exp> \\| <Op1> <Exp> \\| <int> \\| <char> \\| '\textbf{False}' \\| '\textbf{True}' \\| '(' <Exp> ')' \\| <FunCall> \\| '[]' \\| '(' <Exp> ',' <Exp> ')'

	<Field> = [ <Field> ( '.' '\textbf{hd}' | '.' '\textbf{tl}' | '.' '\textbf{fst}' | '.' '\textbf{snd}' ) ]

	<FunCall> = <id> '(' [ <ActArgs> ] ')'

	<ActArgs> = <Exp> [ ',' <ActArgs> ]

	<Op2> = '+' | '-' | '*' | '/' | '\%' | '==' | '\textless' | '\textgreater' | '\textless=' | '\textgreater=' | '!=' | '\&\&' | '||' | ':'

	<Op1> = '!' | '-'

	<int> = [ '-' ] digit+

	<char> = ' alpha '

	<id> = alpha ( '\_' | alphaNum)*
\end{grammar}

\subsection{Parser}
To implement parsing I decided to use recursive descent parsing. In order to do this in Rust I needed a parser combinator library. The two main choices are \texttt{nom} and \texttt{combine}. I've opted to use \texttt{combine} as it closely mirrors Haskell's \texttt{parsec} which I've tried out before. There were a few hurdles to get it working in Rust. The smaller hurdle was that \texttt{combine} is typically used to parse Strings rather than Tokens. This means that some of the default combinators aren't really useful thus requiring me to write my own. The bigger hurdle was that combinators in \texttt{combine} are defined using Rust macros. This means that there are some compile-time text substitutions in the combinators which abstract away a lot of the type information. This tends to result in really cryptic errors since I have no idea what the underlying types are. This took a bit of time to get used to but towards the end I could typically write a combinator in one try. A minor annoyance was that combinators that fail at some point consume Tokens. So a choice combinator that fails on the first choice may consume input which has the effect that the other variants of the choice combinator will never apply. This often requires wrapping combinators in the try combinator to alleviate this issue. The parser that parses identifiers looks as follows:
\begin{lstlisting}[language=Rust, style=boxed]
parser!{
	fn parse_identifier[I]()(I) -> Ident
	  where [I: Stream<Item=Token>]
	{
		satisfy_map(|x: Token| match x {
			Token::TokenIdentifier(ident) => Some(ident),
			_ => None,
		})
	}
}
\end{lstlisting}

Before implementing the parser I first decided to modify the grammar to solve some issues. These issues are operator precedence, associativity, and left-recursion. While the grammar that was parsed was altered this is barely reflected in the Abstract Syntax Tree. The AST is a straightforward data structure on how one would expect an SPL program to look. The altered grammar can be found below while the final AST (slightly modified for Semantic Analysis) can be found in Appendix~\ref{Appendix2}.

The first change in the grammar is that $\langle$BasicType$\rangle$ has been moved into $\langle$Type$\rangle$. This is merely for convenience and doesn't affect the grammar in any way.

A change that does matter is that $\langle$FArgs$\rangle$ has been changed to remove the left-recursion in it. Left-recursion is a problem for recursive descent parsers as it results in infinite recursion since no Tokens are ever consumed. Luckily changing $\langle$FArgs$\rangle$ to remove left-recursion is trivial. A similar left-recursion can be found in $\langle$Field$\rangle$. To remove left-recursion there $\langle$Field$\rangle$ is split into $\langle$Field$\rangle$ and $\langle$Field2$\rangle$ where $\langle$Field$\rangle$ is simply zero or more $\langle$Field2$\rangle$ Tokens.

The last place where left-recursion is still a problem is in $\langle$Exp$\rangle$ but this part of the grammar also needs to be changed to handle operator precedence and associativity. To solve the operator precedence issue I split $\langle$Exp$\rangle$ for binary operators into multiple $\langle$ExpN$\rangle$ where every $\langle$ExpN$\rangle$ has the next expression at the left side. Since the parser is a recursive descent parser it means that the parser will go into the left branch first which has the result that expressions deeper in the ``ladder'' have a higher priority. Building this laddered structure conveniently also solves the left-recursion issue.

The expression at the right side is for the other side of the operator. This structure has the result that every binary operator is right associative. However, some binary operators are left associative (such as subtraction). This is the reason why for some operators we collect multiple right sides. Imagine the expression \texttt{5 - 4 - 3 - 2 1}. When this reaches the parser for $\langle$Exp5$\rangle$ the result will be \texttt{Lit(Int(5))} for the left side and \texttt{[(TokenMinus, Lit(Int(4))), (TokenMinus, Lit(Int(3))), (TokenMinus, Lit(Int(2))), (TokenMinus, Lit(Int(1)))]} for the right side. Since the right side is a list of expressions we can do a right fold which will result in a single expression that is left associative. This also works in cases where the right side has higher precedence (because that will be done first) and where the right side is right associative.

Using the grammar described the recursive descent parser is fully functional. Error reporting during parsing is a little tricky. \texttt{Combine} returns the Token where a parsing error occurs and since the location of a Token is stored during scanning the compiler can report the error and in what context it happens. However, if combine successfully parses a part of the Token stream it does not result in an error. This has changed in \texttt{Combine 3.0} but that is not stable at this point in time. So it may happen that part of the program compiles fine but a function is in reality actually missing because it had an error somewhere. When it does report an error it looks as follows (missing opening \texttt{(}):
\begin{lstlisting}[language=Rust, style=boxed]
Parser error at line 1. Unexpected token TokenComma:
[(Int, (Bool, Char))] x = 1, (True, 'a')) : [];
\end{lstlisting}

\subsubsection*{Parser Grammar:}
\begin{grammar}
	<SPL> = <Decl>+

	<Decl> = <VarDecl> \\| <FunDecl>

	<VarDecl> = ('\textbf{var}' | <Type>) <id> '=' <Exp> ';'

	<FunDecl> = <id> '(' [ <FArgs> ] ')' [ '::' <FunType> ] '\{' <VarDecl>* <Stmt>+ '\}'

	<RetType> = <Type> \\| '\textbf{Void}'

	<FunType> = [ <FTypes> ] '$\rightarrow$' <RetType>

	<FTypes> = <Type> [ <FTypes> ]

	<Type> = '\textbf{Int}' \\| '\textbf{Bool}' \\| '\textbf{Char}' \\| '(' <Type> ',' <Type> ')' \\| '[' <Type> ']' \\| <id>

	<FArgs> = <id> [ <FArgs> ',' ]

	<Stmt> = '\textbf{if}' '(' <Exp> ')' '\{' <Stmt>* '\}' [ '\textbf{else}' '\{' <Stmt>* '\}' ] \\| '\textbf{while}' '(' <Exp> ')' '\{' <Stmt>* '\}' \\| <id> <Field> '=' <Exp> ';' \\| <FunCall> ';' \\| '\textbf{return}' [ <Exp> ] ';'

	<Exp> = <Exp2> [ '||' <Exp> ]

	<Exp2> = <Exp3> [ '\&\&' <Exp2> ]

	<Exp3> = <Exp4> [ ( '==' | '\textless' | '\textgreater' | '\textless=' | '\textgreater=' | '!=' ) <Exp3> ]

	<Exp4> = <Exp5> ( ( '+' | '-' ) <Exp5> )*

	<Exp5> = <Exp6> ( ( '*' | '/' ) <Exp6> )*

	<Exp6> = <Exp7> ( [ '\%' <Exp7> ] )*

	<Exp7> = <Exp8> [ ':' <Exp7> ]

	<Exp8> = <id> <Field> \\| <Op1> <Exp> \\| <int> \\| <char> \\| '\textbf{False}' \\| '\textbf{True}' \\| '(' <Exp> ')' \\| <FunCall> \\| '[]' \\| '(' <Exp> ',' <Exp> ')'

	<Field> = <Field2>*

	<Field2> = '.' '\textbf{hd}' | '.' '\textbf{tl}' | '.' '\textbf{fst}' | '.' '\textbf{snd}'

	<FunCall> = <id> '(' [ <ActArgs> ] ')'

	<ActArgs> = <Exp> [ ',' <ActArgs> ]

	<Op1> = '!' | '-'

	<int> = [ '-' ] digit+

	<char> = ' alpha '

	<id> = alpha ( '\_' | alphaNum)*
\end{grammar}

\subsection{Pretty-printer}
Implementing the pretty printer is fairly straightforward. After parsing the result is an Abstract Syntax Tree. In order to format this nicely I implement the \texttt{Display} trait for every enum/struct in the AST. Implementing the \texttt{Display} trait involves implementing its associated \texttt{fmt} function which described how the data structure should be printed/displayed. So for something simple as the \texttt{Type} enum the implementation looks as follows:
\begin{lstlisting}[language=Rust, style=boxed]
impl fmt::Display for Type {
	fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
		write!(f, "{}", match *self {
			Type::TInt => String::from("Int"),
			Type::TBool => String::from("Bool"),
			Type::TChar => String::from("Char"),
			Type::TVoid => String::from("Void"),
			Type::TTuple(ref l, ref r) =>
				format!("({}, {})", l, r),
			Type::TList(ref t) => format!("[{}]", t),
			Type::TArrow(ref ftypes, ref rtype) =>
				format!("{}-> {}",
				ftypes.iter().map(|x| format!("{} ", x))
				.collect::<Vec<String>>().concat(), rtype)
		})
	}
}
\end{lstlisting}
For the basic types it simply prints the keyword whereas for composed types it prints the structure around the composed types and then pretty prints the composed types. The result is a nicely printed type.

The most difficult part is managing indentation and ensuring that there are no excessive brackets/braces. Thus for pretty printing the operators which are left associative and the precedence of operators are stored somewhere rather than only describing them through the grammar. The most difficult part is when operators are chained. Currently this is implemented by checking the left and right side of a binary operator and seeing which side has a higher precedence and what the associativity of each side is. The result is a pretty-printer that does indenting and puts braces whenever necessary.

% typing / scoping rules?
\section{Semantic Analysis}
Smenatic Analysis consists of binding time analysis and type checking/inference. For type checking/inference I decided to start with monomorphic type checking and potentially extend to polymorphic type checking if time allowed me to. Unfortunately this did not end up happening and I stuck with monomorphic type checking with the exception of isEmpty in order to make the language somewhat useful. The type checking does not apply for print as it is supposed to be overloaded for every type. Leaving out polymorphic type checking did allow me to implement monomorphic type checking well.

In order to only support monomorphic type checking the grammar had to be changed somewhat. For variable declarations the \texttt{var} keyword is no longer present, function types are no longer optional, and identifiers are removed from the $\langle$Type$\rangle$ construct. This also resulted in minor changes to the AST as some of the \texttt{Type} enums no longer have to be wrapped in \texttt{Option} since they must always exist. Additionally \texttt{TIdent(String)} was removed from the \texttt{Type} enum. The updated grammar can be found below while the final AST can be found in Appendix~\ref{Appendix2}.

For binding time analysis my compiler makes use of a type environment. This type environment stores the names of functions and variables and their corresponding type. Whenever a variable is shadowed by a newer variable declaration it is replaced in the type environment. As a result an identifier only has to be checked for existence in the type environment to determine whether it is in scope or not. The type stored is used to make sure the type of the identifier fits in the context it is used in. A bit of extra care has to be taken whenever a variable is shadowed since one function shadowing a variable should not affect another function not shadowing this variable.

The type checker has a specific order in which it works. It first scans all the function headers and checks if they are correct (correct amount of parameters with the specified types). If they are correct they are entered into the type environment. This can be done without information on anything else and which is why it occurs first.

After that all global variables are checked. A global variable is checked by making sure the specified type at the left side matches the type the right side returns. This has to be done after entering functions into the type environment since expressions at the right side might have function calls which have to be checked.

Checking expressions is fairly easy. If an identifier expression is found it simply checks if the identifier is in the type environment and returns the type. If a binary operator expression is found we simply check if the types of the left and right sides are expected for that binary operator (thus handling overloading). A function call expression is checked by checking all arguments of the function call and seeing if the resulting types of those match the types of the function call. Such procedures are implemented for every expression which has the result that every expression results in a type which can then be used later. One thing that should be noted is that the type of every expression is also separately stored in a \texttt{HashMap} where the key is a pointer to the expression. This is very useful when generating code later.

After checking global variables and entering them into the type environment we can finally check the function bodies as function bodies may rely on global variable declarations. The first thing that is done is that variables are shadowed in the type environment. Variable shadowing does not result in any warning whatsoever as I consider it a normal part of every programming language. After the variable shadowing, and thus checking the variable declarations, the statement are checked.

The easy statements to check are variable assignments, function calls, and return statements. For variable assignments the compiler simply checks if the left side has the same type as the right side. For function calls the same procedure as expressions is taking (in fact the code simply calls the function that analysis expressions). Checking return statements simply involves checking if the type of the expression in the return statement is the same as the return type of the function with some special handling when the return has no expression.

The difficult statements to check are the if/else and while statements. They are difficult because the compiler must ensure that all code paths must result in a return at some point. The procedure to check if/else statements works as follows: The expression within the if is checked to see if it returns a type Bool. Then the contents of the if and else branch are separately checked. Additionally to only checking every branch, the compiler also records if the branch results in a guaranteed return or not. If and only if both branches guarantee a return then the whole if/else statement guarantees a return. This even works when if/else statements are nested. By doing it this way the compiler can know at the end of a function whether there was a guaranteed return beforehand or not. If there wasn't then the compiler will complain that not all paths have a return value. The exception to this is a return with no expression as the function has a return of type Void in that case. Unfortunately this guaranteeing can not be used for while statements. The following code would say that there was a guaranteed return unless the compiler evaluates the expression within the while but that is not possible in all cases. Thus for while statements the compiler simply treats them as never returning.
\begin{lstlisting}[language=Rust, style=boxed]
while(False) {
	return;
}
\end{lstlisting}

After checking the statements in the function bodies type checking is done. If no error occurred anywhere the SPL program is considered correct, otherwise it outputs the error and why it occurs. Two example errors would be:
\begin{lstlisting}[language=Rust, style=boxed]
Global variable type mismatch in:
(Int, Bool) y = x.tl;
Given type '(Int, Bool)', found type '[(Int, Bool)]'.

Return statement 'return;' has type 'Void' while function
expects type 'Int'.
\end{lstlisting}

Something to note about error handling in general. Every stage of the compiler increasingly works on data that is further from the original source code making it harder to have good error messages. In the scanner it is still working directly on the source string. In the parser it is working on Tokens which I annotated with positions for error reporting. During semantic analysis it works on the AST which has no relation to the original source code anymore. To get somewhat around this the pretty-printer is used to print the context.

Briefly mentioned before was the exception to make print and isEmpty polymorphic. As a result print always passes the type checker and isEmpty only passes the type checker as long as a list of some type is given. Since the type of the expressions that are printed is known the code generator can generate code to accommodate this.

\subsubsection*{Semantic Analysis Grammar:}
\begin{grammar}
	<SPL> = <Decl>+

	<Decl> = <VarDecl> \\| <FunDecl>

	<VarDecl> = <Type> <id> '=' <Exp> ';'

	<FunDecl> = <id> '(' [ <FArgs> ] ')' '::' <FunType> '\{' <VarDecl>* <Stmt>+ '\}'

	<RetType> = <Type> \\| '\textbf{Void}'

	<FunType> = [ <FTypes> ] '$\rightarrow$' <RetType>

	<FTypes> = <Type> [ <FTypes> ]

	<Type> = '\textbf{Int}' \\| '\textbf{Bool}' \\| '\textbf{Char}' \\| '(' <Type> ',' <Type> ')' \\| '[' <Type> ']'

	<FArgs> = <id> [ <FArgs> ',' ]

	<Stmt> = '\textbf{if}' '(' <Exp> ')' '\{' <Stmt>* '\}' [ '\textbf{else}' '\{' <Stmt>* '\}' ] \\| '\textbf{while}' '(' <Exp> ')' '\{' <Stmt>* '\}' \\| <id> <Field> '=' <Exp> ';' \\| <FunCall> ';' \\| '\textbf{return}' [ <Exp> ] ';'

	<Exp> = <Exp2> [ '||' <Exp> ]

	<Exp2> = <Exp3> [ '\&\&' <Exp2> ]

	<Exp3> = <Exp4> [ ( '==' | '\textless' | '\textgreater' | '\textless=' | '\textgreater=' | '!=' ) <Exp3> ]

	<Exp4> = <Exp5> ( ( '+' | '-' ) <Exp5> )*

	<Exp5> = <Exp6> ( ( '*' | '/' ) <Exp6> )*

	<Exp6> = <Exp7> ( [ '\%' <Exp7> ] )*

	<Exp7> = <Exp8> [ ':' <Exp7> ]

	<Exp8> = <id> <Field> \\| <Op1> <Exp> \\| <int> \\| <char> \\| '\textbf{False}' \\| '\textbf{True}' \\| '(' <Exp> ')' \\| <FunCall> \\| '[]' \\| '(' <Exp> ',' <Exp> ')'

	<Field> = <Field2>*

	<Field2> = '.' '\textbf{hd}' | '.' '\textbf{tl}' | '.' '\textbf{fst}' | '.' '\textbf{snd}'

	<FunCall> = <id> '(' [ <ActArgs> ] ')'

	<ActArgs> = <Exp> [ ',' <ActArgs> ]

	<Op1> = '!' | '-'

	<int> = [ '-' ] digit+

	<char> = ' alpha '

	<id> = alpha ( '\_' | alphaNum)*
\end{grammar}

\section{Code Generation}
Before I started working on the code generation I first planned how I wanted the memory layout to look like. Before describing the code generator I will describe how the memory is organized as the code generator uses some assumptions that can be done with this specific memory layout.

One of the core insights is that every expression results in a single value. This might be a plain value for basic types such as Int, Bool, and Char or it might be a pointer for types such as Tuples and Lists. I made use of this fact when I decided the memory layout.

The first thing is to decide where to put global variables. These can either be on the stack, on the heap or re-evaluate the expression whenever used. The third option I only mention for the sake of completeness as it would result in weird interactions when global variables are changed (or the global values they depend on are changed). I decided to put them on the stack mainly for the reasons that the amount of global variables is static and that they never go out of scope. Putting them on the stack felt somewhat more permanent than putting them on the heap. So, when the compiler starts it first stores the start of the stack in register R5. Since the code segment is variable the start of the stack needs to be stored somewhere. The reason for storing it in a register is convenience as the compiler can grab it at any time if needed. There is no performance difference as the Simple Stack Machine does not actually put them in registers. So the top of the stack looks like $n$ values where every value is the result of an expression belonging to a global variable.

After the globals the stack frames follow (starting with the stack frame of main). A stack frame starts with $n$ parameters which are the parameters passed by the previous function. These are in reverse order to make addressing relative to the markpointer easier. After the parameters there is the return address. The return address is followed by a word of memory that holds the value of the previous markpointer. The current markpointer point to this word of memory. Then there are $n$ words for the local values. Then there is some scratch space to do computations on (mainly for statements). As mentioned before the calling function is responsible for putting the parameters on the stack. The function that is being called creates the rest of the stack and is also responsible for cleaning up the stack; including the parameters for this function. Having the called function clean up the parameters is not a necessity (the calling function could also have done so) but since it is cleaning up its own stack frame anyway it might as well clean up the parameters as well as they are no longer needed. This has the benefit that after a function call the stack is back to where it started before the function call.

As stated before tuples are stored on the heap. The reason for this is that nested tuples will be a nightmare if they are stored on the stack. The memory layout of tuples is as follows: On the stack there is a pointer to the first element of the tuple which is in the heap. The second element of the tuple is adjacent to the first element on the heap. Thus having a pointer to the first element guarantees being able to find the second element. In SPL tuples are guaranteed to have a size of two which makes this a little easier. The elements on the heap are either the plain value or a pointer to some other place. This allows nested tuples and lists inside tuples. Since type information is known at compile-time the compiler knows whether a plain value or pointer is expected and can generate code accordingly.

Lists are similar to tuples in that they are stored on the heap. For lists I decided to implement them as linked lists instead of having one continuous region of memory dedicated to lists. The main reasoning for this is that it avoids having to copy the entire list whenever adding an element to it. Additionally, I was expecting that to be more tricky than implementing them as linked lists. A pointer to a list (which is on the stack) is thus a pointer to the head of the list. The word adjacent to it is a pointer to the head of the tail of the list. This structure allows infinite length lists (as long as heap size permits naturally). To encode the end of a list the pointer to the tail is 0x0. The address 0x0 is guaranteed to be within the code section so this always works. This has the added benefit that this representation can also be used to represent the empty list since every list has the empty list at the end. Using 0x0 as the pointer is thus a marker for the empty list. Whenever the compiler deals with lists it first has to check the pointer whether it is 0x0 or not.

Consider the following piece of SPL code:
\begin{lstlisting}[language=Rust, style=boxed]
[Int] q = 5 : [];
[Int] x = 1 : 2 : q;
q.hd = 0;
\end{lstlisting}
Since lists are implemented as linked lists this changes \texttt{q} but also \texttt{x}. This is something that should be documented (which I'm doing here). I do not consider this wrong behavior but simply a semantic choice. If this is not desired then a user can write his own function to do a deep copy of a list before adding elements to it.

Now that the memory layout is clear I started implementing code generation. The natural place to start was with expressions as the idea was that they return a single value. For literal expressions this is fairly easy, the compiler simply does a \texttt{ldc} and then the value. For the \texttt{EmptyList} literal it generated two words of 0x0 and stores them on the heap. After that I implemented identifier expressions. These are somewhat tricky as the compiler needs to know from what part of the memory it needs to load them from. They can come from the globals and must thus loaded relative to R5 or they can come from the parameters/locals and must thus be loaded relative to the markpointer. An additional difficulty is that SPL allows users to ask for the head or tail of an identifier. What if a user asks the head of an empty list? The compiler can not always determine this at compile-time thus the compiler must generate code that handles this case at runtime. In my compiler whenever a runtime error occurs it jumps to the RuntimeErr function which simply halts after showing the top value of the stack. This function is overridable by the user if they want some primitive error handling.

Now that the expression building blocks are in place it is rather easy to implement the rest. Take tuples for example, it generates code for each expression and since each expression results in a single value we simply store the two values on the heap. Implementing binary operators works in a similar way. Simply generate code for both sides and do something with the produced values. There is one exception and that is implementing \texttt{==} and \texttt{!=}. Since equality can be used on lists the compiler needs to generate code that can compare two lists of dynamic length. If lists were not implemented as linked lists this would have been significantly easier.

An important thing to mention is the evaluation order. Since function calls put the parameters in reverse order on the stack I decided to evaluate function calls from right to left rather than from left to right and swapping the values. Additionally, operators such as \texttt{||} and \texttt{\&\&} typically have a short-circuit depending on the value of the left side. My compiler does not have this short-circuit. It always evaluates both sides. This was a deliberate choice as I feel in a language that can have side-effects (modifying a global for example) the right-side must get the chance to do it's side-effect. Code that relies on the right-side implicitly not executing is in my opinion poor behavior.

Once expressions are implemented global variables can be implemented by simply generating the code for all the expressions. The values are put on the stack when they are computed so no extra care is required.

After that functions are implemented but without the function body. This allowed me to see whether program flow was working as expected. The code for functions without bodies simply involves creating a label, reserving space for the locals, storing the return value in RR, and cleaning up the stack.

Local variables are very similar to global variables. Simply generate the code that evaluates them and then generate the instruction that stores the top of the stack relative to the markpointer (\texttt{stl}).

Generating code for statements is relatively simple when generating code for expressions is done. The \texttt{FunCall} statement simply generates the same code as the expression but ignores the return value. The \texttt{Return} statement generates the same code as when a function ends but stores the return value in register RR if a value is to be returned. Assignments are a little more tricky as the compiler now needs to write to the place of an identifier rather than copy it to the stack. Luckily the SSM helps with this since it provides the \texttt{ldla} instruction which loads the address instead of the value. The same error handling as before is done if a user creates a runtime error.

Generating the if/else and while statements is not very difficult as they're very normal patterns in assembly like languages. For the if statement simply evaluate the expression and branch if this is False. If it does not branch then we have the code for the statements within the if branch. If it does branch then we have the code for the statements in the else branch. There's a label at the end to make sure the if branch does not also execute the else branch. The while statement is similar except that the expression is evaluated multiple times since it is a loop.

More interesting is the implementation of isEmpty and print as one of them is polymorphic while the other one is overloaded. Implementing isEmpty is fairly easy as the type checker ensures only lists are passed. So isEmpty simply dereference the pointer once and checks if the second word points to 0x0. If it does then it is an empty list, otherwise it isn't.

Implementing print was more tricky. My idea was to implement printing for basic types and then compose print for tuples/lists out of those. Since the types are known at compile-time this is doable. Printing Char and Bool is trivial. For Char a simple \texttt{trap 1} is sufficient. For Bool it translates to a simple if/else where in one branch it prints the text True and in the other branch the text False. Printing Int is unfortunately harder. SSM has a \texttt{trap 0} to print integers; except this can not be used since it adds a newline at the end. Thus I had to implement printing integers myself which is difficult as they have a variable length.

The generated code work as follows: Do modulo 10 to get the remainder. This remainder is put on the stack for printing later. Then divide the original number by 10 to get the non-remainder part. If this is not 0 then we loop back. In the meantime I use register R7 to store how many characters from the stack have to be displayed. If it is 0 then the code simply uses R7 to determine how of \texttt{trap 1} is executed (thus printing the number character by character). Simple example: 1234 mod 10 results in 4 on the stack. 123 $\neq$ 0. 123 mod 10 results in 3 on the stack. 12 $\neq$ 0. 12 mod 10 results in 2 on the stack. 1 $\neq$ 0. 1 mod 10 results in 1 on the stack. 0 = 0. Now \texttt{trap 1} executes 4 times which ends up printing 1234 without a newline at the end. Unfortunately this does not work for negative numbers so there is some extra assembly that makes the number absolute and prints a negative sign if it was negative.

Implementing print for tuples and lists is easy once the basic types are done. They simply print the symbols around the elements and generate the code to print the elements. This works because at compile time we know the type inside the tuples/lists so what code to generate to print the elements is known at compile time. This does mean that print is not a true function but inline assembly for each overloaded variant. An example output of print is the following:
\begin{lstlisting}[language=Rust, style=boxed]
(1, (True, a)) : (-21, (True, a)) : (2, (False, z)) : []
\end{lstlisting}

With that the code generator is complete. It works fine but doesn't generate the best code but that wasn't really the goal. Choosing to implement lists as linked lists proved to be harder and it doesn't really result in much benefit. In terms of memory it uses more since every element now needs two words rather than every element needing one word and some fixed small overhead. In terms of performance it's not faster either except when doing frequent list manipulation.

\section{Generating C code}
For the compiler extension I wanted to be able to use SPL functions from other languages as SPL itself is somewhat limited in functionality. In order to accomplish this SPL needs to compile to real machine code rather than SSM. To me, the most straightforward way to accomplish this was to compile to C code. Compiling SPL to C code allows a C compiler to generate the real machine code. The resulting object file can then be statically linked with other programs allowing SPL functions to be used. Compiling to C code has multiple advantages. It uses the C calling convention making more easily compatible with other languages, it makes SPL somewhat cross-platform, and it produces optimized code.

% describe challenges and how solved
% how does the implementation look

\clearpage
\appendix
\section{Scanner Tokens}
\label{Appendix1}
\begin{lstlisting}[language=Rust, style=boxed]
pub enum Token {
	TokenIdentifier(String),
	TokenInt(i64),
	TokenBool(bool),
	TokenChar(char),
	TokenComment(String),
	TokenVar,
	TokenVoid,
	TokenIntKeyword,
	TokenBoolKeyword,
	TokenCharKeyword,
	TokenIf,
	TokenElse,
	TokenWhile,
	TokenReturn,
	TokenArrow,
	TokenDoubleColon,
	TokenDoubleEquals,
	TokenLtEquals,
	TokenGtEquals,
	TokenNotEquals,
	TokenDoubleAmp,
	TokenDoublePipe,
	TokenEquals,
	TokenParenOpen,
	TokenParenClose,
	TokenBracketOpen,
	TokenBracketClose,
	TokenBraceOpen,
	TokenBraceClose,
	TokenColon,
	TokenSemiColon,
	TokenComma,
	TokenDot,
	TokenPlus,
	TokenMinus,
	TokenMult,
	TokenDiv,
	TokenMod,
	TokenLt,
	TokenGt,
	TokenNot
}
\end{lstlisting}
\section{Final Abstract Syntax Tree}
\label{Appendix2}
\begin{lstlisting}[language=Rust, style=boxed]
pub type Ident = String;

pub struct SPL {
	pub vars: Vec<Variable>,
	pub funs: Vec<Function>
}

pub struct Variable {
	pub name: Ident,
	pub vtype: Type,
	pub value: Expression
}

pub struct Function {
	pub name: Ident,
	pub args: Vec<Ident>,
	pub ftype: Type,
	pub vars: Vec<Variable>,
	pub stmts: Vec<Statement>
}

pub enum Type {
	TInt,
	TBool,
	TChar,
	TVoid,
	TTuple(Box<Type>, Box<Type>),
	TList(Box<Type>),
	TArrow(Vec<Type>, Box<Type>)
}

pub enum Statement {
	If(Expression, Vec<Statement>, Vec<Statement>),
	While(Expression, Vec<Statement>),
	Assignment(Ident, Vec<Field>, Expression),
	FunCall(Expression),
	Return(Option<Expression>)
}

pub enum Expression {
	Ident(Ident, Vec<Field>),
	Op2(Box<Expression>, Op2, Box<Expression>),
	Op1(Op1, Box<Expression>),
	Lit(Literal),
	FunCall(Ident, Vec<Expression>),
	Tuple(Box<Expression>, Box<Expression>)
}

pub enum Field {
	Head,
	Tail,
	First,
	Second
}

pub enum Op2 {
	Addition,
	Subtraction,
	Multiplication,
	Division,
	Modulo,
	Equals,
	LessThan,
	GreaterThan,
	LessEquals,
	GreaterEquals,
	NotEquals,
	And,
	Or,
	Cons
}

pub enum Op1 {
	Not,
	Negation
}

pub enum Literal {
	Int(i64),
	Char(char),
	Bool(bool),
	EmptyList
}
\end{lstlisting}
\end{document}
